"""Bioinformatics Pipeline Runner page for the Streamlit dashboard.

Provides:
- Run Pipeline (upload/search FASTQ + select tool/index + run + track progress + view results)
- Manage Indices (register/list/delete reference indices)
- Job History (recent runs)
"""

from __future__ import annotations

import time
from pathlib import Path
from typing import Any, Dict, List

import streamlit as st

from amprenta_rag.auth.session import get_current_user
from amprenta_rag.database.models import Dataset
from amprenta_rag.database.session import db_session
from amprenta_rag.ingestion.genomics.index_manager import (
    delete_index,
    list_indices,
    register_index,
)
from amprenta_rag.ingestion.genomics.job_runner import get_job_status, list_jobs, submit_job
from amprenta_rag.ingestion.genomics.pipeline import download_fastq, get_ena_fastqs


def _ensure_dir(primary: Path, fallback: Path) -> Path:
    try:
        primary.mkdir(parents=True, exist_ok=True)
        return primary
    except Exception:
        fallback.mkdir(parents=True, exist_ok=True)
        return fallback


UPLOADS_DIR = _ensure_dir(Path("/data/genomics/uploads"), Path("data/genomics/uploads"))


def _user_id() -> str:
    user = get_current_user() or {}
    return str(user.get("id") or "00000000-0000-0000-0000-000000000001")


def _index_label(idx: Any) -> str:
    return f"{idx.organism} | {idx.tool} | {idx.version} ({str(idx.id)[:8]})"


def _read_table_preview(path: Path, max_rows: int = 20) -> tuple[List[str], List[List[str]]]:
    """Return (columns, rows) preview for TSV-like result files."""
    try:
        import pandas as pd  # type: ignore

        df = pd.read_csv(path, sep="\t")
        df2 = df.head(max_rows)
        return list(df2.columns), df2.astype(str).values.tolist()
    except Exception:
        # Fallback: simple tab-splitting
        cols: List[str] = []
        rows: List[List[str]] = []
        try:
            with path.open("rt", encoding="utf-8", errors="replace") as f:
                header = f.readline().rstrip("\n")
                cols = header.split("\t") if header else []
                for _ in range(max_rows):
                    line = f.readline()
                    if not line:
                        break
                    rows.append(line.rstrip("\n").split("\t"))
        except Exception:
            return [], []
        return cols, rows


def _ingest_as_dataset(result_file: str, job_id: str, tool: str) -> str:
    """Create a minimal Dataset record pointing at the result file."""
    with db_session() as db:
        ds = Dataset(
            name=f"Pipeline {tool} {job_id[:8]}",
            omics_type="transcriptomics",
            description=f"Generated by Pipeline Runner job {job_id} ({tool}).",
            file_paths=[result_file],
            dataset_source_type="Pipeline Runner",
            data_origin="Internal â€“ Pipeline",
        )
        db.add(ds)
        db.flush()
        db.refresh(ds)
        return str(ds.id)


def _init_state() -> None:
    st.session_state.setdefault("ena_results", [])
    st.session_state.setdefault("selected_ena_run", None)
    st.session_state.setdefault("current_job_id", None)
    st.session_state.setdefault("uploaded_fastq_path", None)


def _tab_run_pipeline() -> None:
    st.subheader("Run Pipeline")

    # Section 1: Select Input
    st.markdown("### 1) Select Input")
    up_tab, ena_tab = st.tabs(["Upload FASTQ", "Search ENA"])

    with up_tab:
        up = st.file_uploader("Upload FASTQ (.fastq/.fq/.gz)", type=["fastq", "fq", "gz"])
        if up is not None:
            out_path = UPLOADS_DIR / up.name
            try:
                out_path.write_bytes(up.getvalue())
                st.session_state["uploaded_fastq_path"] = str(out_path)
                st.success(f"Saved upload to: {out_path}")
            except Exception as e:  # noqa: BLE001
                st.error(f"Failed to save upload: {e}")

        cur = st.session_state.get("uploaded_fastq_path")
        if cur:
            st.caption(f"Current FASTQ: {cur}")

    with ena_tab:
        q = st.text_input("ENA search keyword", placeholder="e.g., Homo sapiens ALS RNA-seq")
        limit = st.number_input("Max results", min_value=1, max_value=50, value=5, step=1)
        if st.button("Search ENA", type="primary"):
            with st.spinner("Searching ENA..."):
                try:
                    st.session_state["ena_results"] = get_ena_fastqs(str(q), limit=int(limit))
                except Exception as e:  # noqa: BLE001
                    st.error(f"ENA search failed: {e}")
                    st.session_state["ena_results"] = []

        results: List[Dict[str, str]] = st.session_state.get("ena_results") or []
        if results:
            st.success(f"Found {len(results)} runs.")
            st.dataframe(results, use_container_width=True)
            run_ids = [r.get("Run", "") for r in results if r.get("Run")]
            sel = st.selectbox("Select run", run_ids, index=0)
            st.session_state["selected_ena_run"] = next((r for r in results if r.get("Run") == sel), None)

            subset = st.checkbox("Download subset (for testing)", value=True)
            max_lines = st.number_input("Subset max lines (FASTQ)", min_value=100, max_value=20000, value=1000, step=100)
            confirm = st.checkbox("I understand FASTQ downloads can be large", value=False)
            if st.button("Download FASTQ from ENA"):
                run_info = st.session_state.get("selected_ena_run")
                if not run_info:
                    st.error("No ENA run selected.")
                else:
                    with st.spinner("Downloading FASTQ..."):
                        try:
                            p = download_fastq(
                                run_info=run_info,
                                output_dir=UPLOADS_DIR,
                                confirm=bool(confirm),
                                subset=bool(subset),
                                max_lines=int(max_lines),
                            )
                            if p is None:
                                st.warning("Download skipped or failed (confirm must be checked for full downloads).")
                            else:
                                st.session_state["uploaded_fastq_path"] = str(p)
                                st.success(f"Downloaded to: {p}")
                        except Exception as e:  # noqa: BLE001
                            st.error(f"Download failed: {e}")

    # Section 2: Configure Pipeline
    st.markdown("### 2) Configure Pipeline")
    tool = st.selectbox("Tool", ["auto", "salmon", "kallisto"], index=0)

    try:
        indices = list_indices()
    except Exception as e:  # noqa: BLE001
        st.error("Database is missing genomics pipeline tables. Please run migrations.")
        st.code("alembic upgrade head")
        st.caption(f"Details: {e}")
        indices = []
    if not indices:
        st.warning("No indices registered yet. Go to 'Manage Indices' tab to upload/register one.")
        return

    idx_labels = [_index_label(i) for i in indices]
    idx_sel = st.selectbox("Index", idx_labels, index=0)
    idx = indices[idx_labels.index(idx_sel)]

    resolved_tool = idx.tool if tool == "auto" else tool
    st.caption(f"Selected tool: {resolved_tool}")

    # Section 3: Execute
    st.markdown("### 3) Execute")
    fastq_path = st.session_state.get("uploaded_fastq_path")
    run_disabled = not fastq_path
    if st.button("Run", type="primary", disabled=run_disabled, use_container_width=True):
        if not fastq_path:
            st.error("Please upload or download a FASTQ first.")
        else:
            try:
                job_id = submit_job(
                    tool=resolved_tool,
                    fastq_path=str(fastq_path),
                    index_id=str(idx.id),
                    user_id=_user_id(),
                )
                st.session_state["current_job_id"] = str(job_id)
                st.success(f"Submitted job: {job_id}")
            except Exception as e:  # noqa: BLE001
                st.error(f"Failed to submit job: {e}")

    # Progress / Status
    job_id = st.session_state.get("current_job_id")
    if job_id:
        st.markdown("### Status")
        try:
            job = get_job_status(job_id)
            pct = int(job.progress_percent or 0)
            st.progress(max(0, min(100, pct)))
            st.write(f"**Status**: {job.status}  |  **Progress**: {pct}%")
            if job.error_message:
                st.error(job.error_message)
        except Exception as e:  # noqa: BLE001
            st.error(f"Failed to read job status: {e}")
            job = None

        c1, c2, c3 = st.columns(3)
        with c1:
            if st.button("Refresh status"):
                st.rerun()
        with c2:
            auto = st.checkbox("Auto-refresh (1s)", value=False, key="pipeline_autorefresh")
        with c3:
            if st.button("Stop tracking"):
                st.session_state["current_job_id"] = None
                st.rerun()

        if auto and job is not None and job.status in ("pending", "running"):
            time.sleep(1.0)
            st.rerun()

        # Section 4: Results
        if job is not None and job.status == "complete" and job.result_file:
            st.markdown("### 4) Results")
            result_path = Path(job.result_file)
            if not result_path.exists():
                st.warning(f"Result file not found: {result_path}")
                return

            cols, rows = _read_table_preview(result_path, max_rows=20)
            if cols and rows:
                st.dataframe(
                    {"__rows__": list(range(1, len(rows) + 1)), **{c: [r[i] if i < len(r) else "" for r in rows] for i, c in enumerate(cols)}},
                    use_container_width=True,
                )
            else:
                st.info("Could not parse result preview.")

            try:
                data = result_path.read_bytes()
                st.download_button(
                    "Download TSV",
                    data=data,
                    file_name=result_path.name,
                    mime="text/tab-separated-values",
                    use_container_width=True,
                )
            except Exception as e:  # noqa: BLE001
                st.error(f"Failed to load result for download: {e}")

            if st.button("Ingest as Dataset", use_container_width=True):
                try:
                    ds_id = _ingest_as_dataset(result_file=str(result_path), job_id=str(job.id), tool=str(job.tool))
                    st.success(f"Ingested as Dataset: {ds_id}")
                except Exception as e:  # noqa: BLE001
                    st.error(f"Failed to ingest dataset: {e}")


def _tab_manage_indices() -> None:
    st.subheader("Manage Indices")

    st.markdown("### Upload / Register Index")
    with st.form("register_index_form", clear_on_submit=True):
        organism = st.text_input("Organism", value="Homo sapiens")
        tool = st.selectbox("Tool", ["salmon", "kallisto"], index=0)
        version = st.text_input("Version", value="GRCh38_v110")
        f = st.file_uploader("Index file (e.g., tar.gz or index bundle)", type=["gz", "tar", "tgz", "zip"])
        submit = st.form_submit_button("Register Index", type="primary")

    if submit:
        if not (organism.strip() and tool.strip() and version.strip() and f is not None):
            st.error("Please provide organism, tool, version, and a file.")
        else:
            tmp_path = UPLOADS_DIR / f.name
            try:
                tmp_path.write_bytes(f.getvalue())
                with st.spinner("Registering index..."):
                    idx = register_index(
                        organism=organism.strip(),
                        tool=tool.strip(),
                        version=version.strip(),
                        file_path=str(tmp_path),
                        user_id=_user_id(),
                    )
                st.success(f"Registered index: {idx.id}")
            except Exception as e:  # noqa: BLE001
                st.error(f"Failed to register index: {e}")

    st.markdown("### Existing Indices")
    try:
        indices = list_indices()
    except Exception as e:  # noqa: BLE001
        st.error("Database is missing genomics pipeline tables. Please run migrations.")
        st.code("alembic upgrade head")
        st.caption(f"Details: {e}")
        indices = []
    if not indices:
        st.info("No indices registered yet.")
        return

    for idx in indices:
        with st.expander(_index_label(idx), expanded=False):
            st.write(
                {
                    "id": str(idx.id),
                    "organism": idx.organism,
                    "tool": idx.tool,
                    "version": idx.version,
                    "file_path": idx.file_path,
                    "file_size_bytes": idx.file_size_bytes,
                    "uploaded_at": str(idx.uploaded_at),
                }
            )
            if st.button("Delete index", key=f"delete_index_{idx.id}", type="secondary"):
                ok = delete_index(str(idx.id))
                if ok:
                    st.success("Deleted.")
                    st.rerun()
                else:
                    st.warning("Index not found.")


def _tab_job_history() -> None:
    st.subheader("Job History")
    try:
        jobs = list_jobs(user_id=_user_id(), limit=20)
    except Exception as e:  # noqa: BLE001
        st.error("Database is missing genomics pipeline tables. Please run migrations.")
        st.code("alembic upgrade head")
        st.caption(f"Details: {e}")
        jobs = []
    if not jobs:
        st.info("No jobs found.")
        return

    for job in jobs:
        title = f"{str(job.id)[:8]} | {job.tool} | {job.status} | {job.progress_percent or 0}%"
        with st.expander(title, expanded=False):
            st.write(
                {
                    "id": str(job.id),
                    "status": job.status,
                    "tool": job.tool,
                    "input_fastq_path": job.input_fastq_path,
                    "index_id": str(job.index_id) if job.index_id else None,
                    "output_dir": job.output_dir,
                    "result_file": job.result_file,
                    "progress_percent": job.progress_percent,
                    "error_message": job.error_message,
                    "started_at": str(job.started_at) if job.started_at else None,
                    "completed_at": str(job.completed_at) if job.completed_at else None,
                    "created_at": str(job.created_at) if job.created_at else None,
                }
            )
            if job.result_file:
                p = Path(job.result_file)
                if p.exists():
                    try:
                        st.download_button(
                            "Download result",
                            data=p.read_bytes(),
                            file_name=p.name,
                            mime="text/tab-separated-values",
                            key=f"download_job_{job.id}",
                        )
                    except Exception:
                        pass


def render_pipeline_runner_page() -> None:
    """Render the Pipeline Runner page."""
    from scripts.dashboard.auth import require_auth

    require_auth()  # gate expensive compute resources behind auth
    _init_state()

    st.header("ðŸ§¬ Pipeline Runner")
    st.caption("Run Salmon/Kallisto quantification, manage indices, and track jobs.")

    tab1, tab2, tab3 = st.tabs(["Run Pipeline", "Manage Indices", "Job History"])
    with tab1:
        _tab_run_pipeline()
    with tab2:
        _tab_manage_indices()
    with tab3:
        _tab_job_history()


__all__ = ["render_pipeline_runner_page"]


